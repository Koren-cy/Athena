---
tags:
  - 数学
dlink:
  - "[[--概率论--]]"
  - "[[方差分析及回归]]"
---
一元线性回归是一种统计方法，用于分析两个连续变量之间的线性关系。它通过拟合一条直线来最小化预测值与实际值之间的误差。具体来说，一元线性回归模型可以描述如下：

### 一元线性回归模型
一元线性回归模型的形式为：
$$ y = \beta_0 + \beta_1 x + \epsilon $$

- $y$ 是因变量（也称为响应变量或被解释变量）。
- $x$ 是自变量（也称为解释变量或预测变量）。
- $\beta_0$ 是截距项，表示当 $x = 0$ 时 $y$ 的期望值。
- $\beta_1$ 是斜率项，表示 $x$ 每增加一个单位， $y$ 的期望值增加的单位。
- $\epsilon$ 是误差项，表示实际值与预测值之间的差异，通常假设服从均值为0的正态分布，即 $\epsilon \sim N(0, \sigma^2)$。

### 最小二乘法
为了估计回归系数 $\beta_0$ 和 $\beta_1$，通常使用最小二乘法（Least Squares Method）。该方法通过最小化以下残差平方和（Residual Sum of Squares，RSS）来确定最优的回归系数：
$$ RSS = \sum_{i=1}^n (y_i - (\beta_0 + \beta_1 x_i))^2 $$

通过求解这个最小化问题，可以得到回归系数的估计值：
$$ \hat{\beta}_1 = \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^n (x_i - \bar{x})^2} $$
$$ \hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x} $$

其中， $\bar{x}$ 和 $\bar{y}$ 分别是 $x$ 和 $y$ 的均值。

### 模型评估
为了评估一元线性回归模型的拟合效果，常用以下几种统计量：

#### 决定系数 ($R^2$)
$$ R^2 = 1 - \frac{RSS}{TSS} $$
其中， $TSS$ 是总平方和（Total Sum of Squares），表示 $y$ 的总变异：
$$ TSS = \sum_{i=1}^n (y_i - \bar{y})^2 $$

$R^2$ 取值范围为0到1，值越接近1，表示模型对数据的解释能力越强。

#### 均方误差 (MSE)
$$ MSE = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2 $$
其中， $\hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1 x_i$ 是预测值。

### 假设检验
为了检验回归系数是否显著，可以进行假设检验，例如：
- 检验 $\beta_1$ 是否为零（即自变量 $x$ 对因变量 $y$ 是否有显著影响）。通常使用 t 检验。

### Python 实现示例
以下是使用Python实现一元线性回归的示例代码：

```python
import numpy as np
import matplotlib.pyplot as plt

# 生成示例数据
x = np.array([1, 2, 3, 4, 5])
y = np.array([2, 4, 5, 4, 5])

# 计算均值
x_mean = np.mean(x)
y_mean = np.mean(y)

# 计算回归系数
numerator = np.sum((x - x_mean) * (y - y_mean))
denominator = np.sum((x - x_mean) ** 2)
beta_1 = numerator / denominator
beta_0 = y_mean - beta_1 * x_mean

# 打印回归系数
print(f'regression coefficient β0: {beta_0}')
print(f'regression coefficient β1: {beta_1}')

# 预测值
y_pred = beta_0 + beta_1 * x

# 绘制数据点和回归线
plt.scatter(x, y, color='blue', label='data scatter')
plt.plot(x, y_pred, color='red', label='regression line')
plt.xlabel('x')
plt.ylabel('y')
plt.show()
```
