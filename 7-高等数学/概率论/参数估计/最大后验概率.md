---
tags:
  - 数学
dlink:
  - "[[---参数估计---]]"
aliases:
  - MAP
  - Maximum A Posteriori
  - 事後確率最大化
---
事后概率最大化(MAP)是一种估计参数的方法, 它结合了观测数据 ([[似然函数]]) 和先验分布来估计参数. 与[[最大似然估计]] (MLE) 不同, MAP 考虑了先验信息. 

在 MAP 中, 我们试图最大化事后概率分布: 
$$
p(\theta | \text{data}) \propto p(\text{data} | \theta) p(\theta)
$$
其中 $\theta$ 是参数, $p(\text{data} | \theta)$ 是尤度函数, $p(\theta)$ 是先验分布. 

为了计算 MAP 估计, 我们最大化对数事后概率: 
$$
\log p(\theta | \text{data}) \propto \log p(\text{data} | \theta) + \log p(\theta)
$$

---
## 最大后验概率估计（MAP）

在贝叶斯统计学中，“最大后验概率估计”是后验概率分布的众数。利用最大后验概率估计可以获得对实验数据中无法直接观察到的量的点估计。它与最大似然估计中的经典方法有密切关系，但是它使用了一个增广的优化目标，进一步考虑了被估计量的先验概率分布。所以最大后验概率估计可以看作是规则化（regularization）的最大似然估计。

### 定义
假设我们需要根据观察数据 $x$ 估计未观察到的总体参数 $\theta$，让 $f$ 作为 $x$ 的采样分布，这样 $f(x|\theta)$ 就是总体参数为 $\theta$ 时 $x$ 的概率。函数

$$
\theta \mapsto f(x|\theta)
$$

即为似然函数，其估计

$$
\hat{\theta}_{\text{ML}}(x) = \arg \max_{\theta} f(x|\theta)
$$

就是 $\theta$ 的最大似然估计。

假设 $\theta$ 存在一个先验分布 $g$，这就允许我们将 $\theta$ 作为贝叶斯统计中的随机变量，这样 $\theta$ 的后验分布就是：

$$
\theta \mapsto \frac{f(x|\theta) g(\theta)}{\int_{\Theta} f(x|\theta') g(\theta') d\theta'}
$$

其中 $\Theta$ 是 $g$ 的定义域，这是贝叶斯定理的直接应用。

### 最大后验概率估计方法

于是估计 $\theta$ 为这个随机变量的后验分布的众数：

$$
\hat{\theta}_{\text{MAP}}(x) = \arg \max_{\theta} \frac{f(x|\theta) g(\theta)}{\int_{\Theta} f(x|\theta') g(\theta') d\theta'} = \arg \max_{\theta} f(x|\theta) g(\theta)
$$

后验分布的分母与 $\theta$ 无关，所以在优化过程中不起作用。注意当先验 $g$ 是常数函数时，最大后验估计与最大似然估计重合。

### 计算最大后验估计的方法

1. **解析方法**：当后验分布的模能够用解析解方式表示时使用这种方法。当使用共轭先验时就是这种情况。
2. **数值优化方法**：如共轭梯度法或者牛顿法，这通常需要一阶或二阶导数，导数需要通过解析或数值方法得到。
3. **期望最大化算法的修改**：这种方法不需要后验密度的导数。

### 注意事项

尽管最大后验估计与贝叶斯统计共享先验分布的使用，通常并不认为它是一种贝叶斯方法，这是因为最大后验估计是点估计。然而，贝叶斯方法的特点是使用这些分布来总结数据、得到推论。贝叶斯方法试图算出后验均值或者中值以及后验区间，而不是后验模。尤其是当后验分布没有一个简单的解析形式时更是如此。在这种情况下，后验分布可以使用马尔可夫链蒙特卡罗（Markov Chain Monte Carlo, MCMC）技术来模拟，但是找到它的众数的优化是很困难或者是不可能的。

### 示例

假设我们有观测数据 $y$，模型为 $y = a_1 x_1 + a_2 x_2 + e$，其中 $e$ 是服从标准正态分布 $N(0, 1)$ 的误差项。

观测数据为：
$$
\begin{cases}
a_1 = 1, a_2 = 0, y = -5 \\
a_1 = 0, a_2 = 1, y = 7 \\
a_1 = 1, a_2 = 1, y = 1 \\
\end{cases}
$$

假设 $x_1$ 和 $x_2$ 的先验分布为 $x_1 - x_2$ 服从均值为 1，方差为 2 的正态分布：
$$
p(x_1, x_2) \propto \exp\left(-\frac{(x_1 - x_2 - 1)^2}{4}\right)
$$

观测数据的尤度函数为：
$$
L(x_1, x_2) \propto \exp\left(-\frac{1}{2}\left[(-5 - x_1)^2 + (7 - x_2)^2 + (1 - (x_1 + x_2))^2\right]\right)
$$

结合先验分布和尤度函数得到事后分布：
$$
p(x_1, x_2 | y) \propto \exp\left(-\frac{1}{2}\left[(-5 - x_1)^2 + (7 - x_2)^2 + (1 - (x_1 + x_2))^2\right]\right) \cdot \exp\left(-\frac{(x_1 - x_2 - 1)^2}{4}\right)
$$

取对数以简化计算：
$$
\log p(x_1, x_2 | y) \propto -\frac{1}{2}\left[(-5 - x_1)^2 + (7 - x_2)^2 + (1 - (x_1 + x_2))^2\right] - \frac{(x_1 - x_2 - 1)^2}{4}
$$

对 $x_1$ 和 $x_2$ 求偏导数并设为零可以找到最大后验概率估计：
$$
\frac{\partial \log p(x_1, x_2 | y)}{\partial x_1} = (x_1 + 5) + (x_1 + x_2 - 1) - \frac{1}{2}(x_1 - x_2 - 1) = 0
$$

$$
\frac{\partial \log p(x_1, x_2 | y)}{\partial x_2} = (x_2 - 7) + (x_1 + x_2 - 1) + \frac{1}{2}(x_1 - x_2 - 1) = 0
$$

求解这两个方程可以得到 $x_1$ 和 $x_2$ 的最大后验估计。



### 最大后验概率估计

在最大后验概率估计中，我们感兴趣的是寻找参数 $\theta$ 使得后验概率 $p(\theta | \text{data})$ 最大化。贝叶斯定理告诉我们：

$$
p(\theta | \text{data}) = \frac{p(\text{data} | \theta) p(\theta)}{p(\text{data})}
$$

其中：

- $p(\theta | \text{data})$ 是在给定数据的情况下参数 $\theta$ 的后验概率。
- $p(\text{data} | \theta)$ 是在参数 $\theta$ 给定的情况下观测数据的似然（Likelihood）。
- $p(\theta)$ 是参数 $\theta$ 的先验概率（Prior）。
- $p(\text{data})$ 是观测数据的边缘似然（Marginal Likelihood），也是一个归一化常数。

在最大化后验概率时，$p(\text{data})$ 是一个常数，与 $\theta$ 无关。由于 $p(\text{data})$ 对于每一个可能的 $\theta$ 都是相同的，所以它不会影响最大化过程。因此，我们可以忽略分母 $p(\text{data})$，直接最大化分子的部分：

$$
p(\theta | \text{data}) \propto p(\text{data} | \theta) p(\theta)
$$

### 为什么忽略 $p(\text{data})$

1. **常数因子不影响最大化过程**：在最大化后验概率时，$p(\text{data})$ 作为一个常数，只影响概率的绝对值，不影响概率分布的形状。因此，我们可以忽略它，仅仅最大化 $p(\text{data} | \theta) p(\theta)$。

2. **简化计算**：计算 $p(\text{data})$ 往往比较复杂，因为它是对所有可能的参数 $\theta$ 进行积分或求和。通过忽略它，我们可以简化计算过程，只需要考虑相对容易计算的部分。

### 最大后验概率（MAP）估计过程

在 MAP 估计中，我们寻找 $\theta$ 使得后验概率最大化：

$$
\theta_{MAP} = \arg\max_{\theta} p(\theta | \text{data})
$$

由于

$$
p(\theta | \text{data}) \propto p(\text{data} | \theta) p(\theta)
$$

我们可以简化为：

$$
\theta_{MAP} = \arg\max_{\theta} p(\text{data} | \theta) p(\theta)
$$

这个过程分为两个部分：

1. **似然（Likelihood）**：$p(\text{data} | \theta)$ 表示在参数 $\theta$ 给定的情况下，观测数据出现的概率。它反映了数据与参数 $\theta$ 的一致性。
2. **先验（Prior）**：$p(\theta)$ 表示我们在没有看到数据之前对参数 $\theta$ 的先验信念。它反映了我们对参数 $\theta$ 的先验知识或假设。

通过最大化 $p(\text{data} | \theta) p(\theta)$，我们同时考虑了数据和先验知识，从而得到更合理的参数估计。

### 例子

假设我们有一个正态分布的观测数据集，我们希望估计其均值 $\mu$，且假设我们对 $\mu$ 的先验分布也是正态分布。

1. **似然函数**：假设观测数据为 $x_1, x_2, \ldots, x_n$，每个数据点独立同分布于均值为 $\mu$、方差为 $\sigma^2$ 的正态分布，则似然函数为：

$$
p(\text{data} | \mu) = \prod_{i=1}^n \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(-\frac{(x_i - \mu)^2}{2 \sigma^2}\right)
$$

2. **先验分布**：假设 $\mu$ 的先验分布为均值为 $\mu_0$、方差为 $\tau^2$ 的正态分布，则先验分布为：

$$
p(\mu) = \frac{1}{\sqrt{2 \pi \tau^2}} \exp\left(-\frac{(\mu - \mu_0)^2}{2 \tau^2}\right)
$$

3. **后验分布**：根据贝叶斯定理，后验分布为：

$$
p(\mu | \text{data}) \propto p(\text{data} | \mu) p(\mu)
$$

我们可以忽略归一化常数，直接最大化后验分布的分子部分：

$$
p(\mu | \text{data}) \propto \left(\prod_{i=1}^n \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(-\frac{(x_i - \mu)^2}{2 \sigma^2}\right)\right) \cdot \frac{1}{\sqrt{2 \pi \tau^2}} \exp\left(-\frac{(\mu - \mu_0)^2}{2 \tau^2}\right)
$$

通过取对数并简化，我们可以得到 $\mu$ 的最大后验估计 $\mu_{MAP}$。

总之，忽略 $p(\text{data})$ 使得计算更加简便，而不会影响最大化过程的结果。这是 MAP 估计中的常见做法。